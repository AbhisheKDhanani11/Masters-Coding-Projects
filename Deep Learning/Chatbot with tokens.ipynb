{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8010ca-e1d3-4b86-a7e2-a639b14b2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 10:53:08.193942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325a2817-85f6-4453-9926-02931d0e5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded dataset\n",
    "input_texts = [\n",
    "    'hi', 'hello', 'how are you', 'bye', 'what is your name', 'thanks',\n",
    "    'what do you do', 'who made you', 'how old are you', 'what is ai',\n",
    "    'tell me a joke', 'what is the weather', 'good morning', 'good night',\n",
    "    'are you real', 'i love you', 'can you help me', 'what is your job',\n",
    "    'where are you from', 'do you like humans',\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    'hello', 'hi', 'i am fine, thank you', 'goodbye', 'i am a bot',\n",
    "    'you are welcome', 'i help users with questions', 'i was made by OpenAI',\n",
    "    'i do not age', 'ai means artificial intelligence',\n",
    "    'why did the computer get cold? because it left its windows open!',\n",
    "    'i cannot check the weather, but i hope it’s sunny!',\n",
    "    'good morning!', 'good night!', 'not really, just code',\n",
    "    'i love chatting with you', 'yes, how can i assist you?',\n",
    "    'my job is to talk with you', 'i live on the internet',\n",
    "    'i think humans are amazing!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d581fb-5353-4e3d-9c62-0d9e982c56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <start> and <end> tokens to target sentences\n",
    "target_texts = ['<start> ' + text + ' <end>' for text in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "270cee40-ee2c-44f0-8784-c39e461608ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target texts\n",
    "input_tokenizer = Tokenizer()\n",
    "target_tokenizer = Tokenizer(filters='')  # Keeps special tokens like <start> and <end>\n",
    "\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "max_encoder_seq_length = max(len(seq) for seq in input_sequences)\n",
    "max_decoder_seq_length = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=max_decoder_seq_length-1, padding='post')\n",
    "decoder_target_data = pad_sequences([seq[1:] for seq in target_sequences], maxlen=max_decoder_seq_length-1, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64700826-9297-4ce2-adb9-6a88e7093f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary sizes\n",
    "num_encoder_tokens = len(input_tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf08de50-2e92-4eff-99d9-be0b8c77a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode decoder target data\n",
    "decoder_target_data_oh = tf.keras.utils.to_categorical(decoder_target_data, num_classes=num_decoder_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69cefefa-673e-45c8-8476-99c4cd65486b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x162d77f80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Seq2Seq model\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data_oh, batch_size=2, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fff5388-7506-44a8-9ca8-455e99ba2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models\n",
    "# Encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_embedding2 = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca562907-a868-4026-8826-da3d893c1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup for words\n",
    "reverse_target_index = {i: word for word, i in target_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_seq])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoder_seq_length, padding='post')\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array([[target_tokenizer.word_index['<start>']]])\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_index.get(sampled_token_index, '')\n",
    "    \n",
    "        if sampled_word == '<end>' or sampled_word == '':\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "    \n",
    "            target_seq = np.array([[sampled_token_index]])\n",
    "            states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c503b9-3d24-450c-b271-cfc4ef2ee170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Bot: hello\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how are you \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Bot: i am fine, thank\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Bot: i am a bot\n"
     ]
    }
   ],
   "source": [
    "# Chat loop\n",
    "print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").lower()\n",
    "    if user_input == 'exit':\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    response = decode_sequence(user_input)\n",
    "    print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8effb09-29f2-443b-a325-0b99db502cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
