{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdce58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cappr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66553a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "fake_df = pd.read_csv(\"D:/Masters/Sem 2/AI/Project/Dataset/archive/Fake.csv\")\n",
    "true_df = pd.read_csv(\"D:/Masters/Sem 2/AI/Project/Dataset/archive/True.csv\")\n",
    "\n",
    "fake_df[\"label\"] = 1\n",
    "true_df[\"label\"] = 0\n",
    "\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "combined_df = combined_df[[\"text\", \"label\"]]\n",
    "\n",
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc30556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Convertting to Dataset\n",
    "dataset = Dataset.from_pandas(combined_df)\n",
    "print(type(dataset))\n",
    "\n",
    "# Shuffle the full dataset\n",
    "shuffled_dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70cbea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size after cleaning: 44898\n"
     ]
    }
   ],
   "source": [
    "# Calculate total size\n",
    "total_size = len(shuffled_dataset)\n",
    "print(f\"Total size after cleaning: {total_size}\")\n",
    "\n",
    "# Splitting into train, validation, and test sets (70% train, 15% val, 15% test)\n",
    "train_val_test_split = shuffled_dataset.train_test_split(test_size=0.3, seed=42)  # 70% train, 30% test\n",
    "val_test_split = train_val_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)  # 15% val, 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e45bc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 31428\n",
      "Validation size: 6735\n",
      "Test size: 6735\n",
      "Sample: {'text': 'A protester tried to attack Trump today at a rally in Dayton, OH but was subdued by the Secret Service. The crowd went nuts! Trump s reaction is PRICELESS:', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"train\": train_val_test_split[\"train\"],      # 70% of total_size\n",
    "    \"validation\": val_test_split[\"train\"],       # 15% of total_size\n",
    "    \"test\": val_test_split[\"test\"]              # 15% of total_size \n",
    "}\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "print(\"Sample:\", dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7228615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# tokenizer functions\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,  \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8d1397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51de9bbdafb4c679fc3a26c2601f406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208d75d727294b8780f3a9de3c82f8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76534eefd0e74549a9752e7f4d0fd2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A protester tried to attack Trump today at a rally in Dayton, OH but was subdued by the Secret Service. The crowd went nuts! Trump s reaction is PRICELESS:', 'label': 1, 'input_ids': [101, 1037, 6186, 2121, 2699, 2000, 2886, 8398, 2651, 2012, 1037, 8320, 1999, 14700, 1010, 2821, 2021, 2001, 20442, 2011, 1996, 3595, 2326, 1012, 1996, 4306, 2253, 12264, 999, 8398, 1055, 4668, 2003, 3976, 3238, 1024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset by mapping the fuctions\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(preprocess_function, batched=True),\n",
    "    \"validation\": dataset[\"validation\"].map(preprocess_function, batched=True),\n",
    "    \"test\": dataset[\"test\"].map(preprocess_function, batched=True)\n",
    "}\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721b8382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train split: {'label': tensor(1), 'input_ids': tensor([  101,  1037,  6186,  2121,  2699,  2000,  2886,  8398,  2651,  2012,\n",
      "         1037,  8320,  1999, 14700,  1010,  2821,  2021,  2001, 20442,  2011,\n",
      "         1996,  3595,  2326,  1012,  1996,  4306,  2253, 12264,   999,  8398,\n",
      "         1055,  4668,  2003,  3976,  3238,  1024,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Setting format for PyTorch\n",
    "for split in tokenized_dataset:\n",
    "    tokenized_dataset[split].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# sample\n",
    "print(\"Sample from train split:\", tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e55c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Loading the DistilBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2  # Binary classification: 0 (real), 1 (fake)\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Verify itâ€™s on CUDA\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379372a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Where to save model checkpoints\n",
    "    eval_strategy=\"epoch\",              # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                 # Standard learning rate for BERT models\n",
    "    per_device_train_batch_size=8,      # Batch size for training \n",
    "    per_device_eval_batch_size=8,       # Batch size for evaluation\n",
    "    num_train_epochs=1,                 # Number of epochs to train\n",
    "    weight_decay=0.01,                  # Regularization to prevent overfitting\n",
    "    logging_dir=\"./logs\",               # Where to save training logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    fp16=True,                          # Mixed precision for faster training on GPU\n",
    "    save_strategy=\"epoch\",              # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,        # Load the best model based on validation\n",
    "    metric_for_best_model=\"accuracy\",   # Use accuracy to pick the best model\n",
    "    report_to=\"none\"                    # Disable WandB to avoid overhead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca17955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a compute_metrics function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d57bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cappr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c6d7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3929' max='3929' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3929/3929 32:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.999109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 0.0001681193825788796, 'eval_accuracy': 1.0, 'eval_runtime': 126.1276, 'eval_samples_per_second': 53.398, 'eval_steps_per_second': 6.676, 'epoch': 1.0}\n",
      "[[3192    0]\n",
      " [   0 3543]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       1.00      1.00      1.00      3192\n",
      "        Fake       1.00      1.00      1.00      3543\n",
      "\n",
      "    accuracy                           1.00      6735\n",
      "   macro avg       1.00      1.00      1.00      6735\n",
      "weighted avg       1.00      1.00      1.00      6735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Test set evaluation:\", eval_results)\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "labels = predictions.label_ids\n",
    "print(confusion_matrix(labels, preds))\n",
    "print(classification_report(labels, preds, target_names=[\"Real\", \"Fake\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf1241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
